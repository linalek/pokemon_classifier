{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8D3I-2ut6AT"
   },
   "source": [
    "# Pokémon images classification challenge: Task 2: CNN from scratch to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhXkY-Y6uMEn"
   },
   "source": [
    "## 1. Previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xc6kz4ZrjZ9"
   },
   "source": [
    "- Introduction\n",
    "- EDA (Exploratory Data Analysis) of the dataset\n",
    "- pre-processing of the data\n",
    "- classification using an MLP (Multilayer Perceptron).\n",
    "\n",
    "This work is available at this link: https://drive.google.com/file/d/1fci5SJnuwGc3tGgtzdE0X4BfafhX83B3/view?usp=sharing\n",
    "\n",
    "In this notebook, we perform a classification using a **CNN (Convolutional Neural Network)**. The dataset consists of **images of Pokémon**, each identified by an ID (corresponding to an image file with png extension) and a label indicating its **type**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaqGgcJ7rxsL"
   },
   "source": [
    "## 2. Setting up the environment and the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqfLDf78deXu"
   },
   "source": [
    "### Librairie importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ7kHl-pddy2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHjw3sqTsAU2"
   },
   "source": [
    "### Data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "executionInfo": {
     "elapsed": 5081,
     "status": "error",
     "timestamp": 1744571325635,
     "user": {
      "displayName": "Lina Lekbouri",
      "userId": "13756538654698639069"
     },
     "user_tz": -60
    },
    "id": "Tf2ST3e8d4bg",
    "outputId": "737c1e34-8c36-4153-e65c-d9cd463bdf94"
   },
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "Error: credential propagation was unsuccessful",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-52c4dd70687c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#use of google drive to import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
     ]
    }
   ],
   "source": [
    "# #use of google drive to import data\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1744571328795,
     "user": {
      "displayName": "Lina Lekbouri",
      "userId": "13756538654698639069"
     },
     "user_tz": -60
    },
    "id": "7Y9uoZQ7eWZi",
    "outputId": "65bb5e2d-4601-4902-fb4a-ce7a691117a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'drive/MyDrive/AP_Assignement1_Task2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# ! ls drive/MyDrive/AP_Assignement1_Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvlbYd5HeDLJ"
   },
   "outputs": [],
   "source": [
    "#choosing the path where the dataset is stocked\n",
    "\n",
    "# # Google Colab\n",
    "# my_path = 'drive/MyDrive/AP_Assignement1_Task3/'\n",
    "\n",
    "# Kaggle\n",
    "# my_path = '/kaggle/input/the-pokemon-are-out-there-task-1/'\n",
    "\n",
    "# Local\n",
    "my_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdPOYYfYvmn9"
   },
   "source": [
    "## 3. Data prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snYOJmYZf9-N"
   },
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-odVZk3XgAnC"
   },
   "outputs": [],
   "source": [
    "# Load labels\n",
    "data_path = my_path +\"train_labels.csv\"\n",
    "image_folder = my_path +\"Train\"\n",
    "labels_df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1744063721233,
     "user": {
      "displayName": "Hugo Werck",
      "userId": "15023081001397889950"
     },
     "user_tz": -60
    },
    "id": "iumqx2hdggiX",
    "outputId": "51af87f3-0dc2-4f01-8d1c-3effe342ec0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-219a7947c3f6>:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = labels_df.groupby('label').apply(lambda x: x.sample(min_class_count)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#undersampling to balance the data\n",
    "min_class_count = labels_df['label'].value_counts().min()\n",
    "balanced_df = labels_df.groupby('label').apply(lambda x: x.sample(min_class_count)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_40PxIUTjcQ-"
   },
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSXsHx1Ijfd1"
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), #assure all images are 64x64\n",
    "    transforms.ToTensor(), #convert to tensor pytorch\n",
    "    transforms.Normalize([0.4464, 0.4480, 0.4158],[0.1823, 0.1728, 0.1813], inplace=False),\n",
    "    transforms.RandomHorizontalFlip(), #data augmentation, we don't use the crop because of our data preprocessing, since the crop is random it can remove the pokémon\n",
    "    #and affects our performance\n",
    "    transforms.RandomRotation(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 592093,
     "status": "error",
     "timestamp": 1744064313387,
     "user": {
      "displayName": "Hugo Werck",
      "userId": "15023081001397889950"
     },
     "user_tz": -60
    },
    "id": "E83VDJ48kX5K",
    "outputId": "b1c69bac-626a-419b-a33e-c7441042001f"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-861f1adc4408>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Load train and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-861f1adc4408>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(df, img_folder, transform)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Construct full image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Open image in RGB mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3474\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3476\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "\n",
    "# Create label-to-index mapping\n",
    "unique_labels = train_df[\"label\"].unique()\n",
    "label_map = {label: Id for Id, label in enumerate(unique_labels)}\n",
    "\n",
    "# Function to load images and labels into tensors\n",
    "def load_dataset(df, img_folder, transform):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(img_folder, row[\"Id\"] + \".png\")  # Construct full image path\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Open image in RGB mode\n",
    "        image = transform(image)  # Apply transformations\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(label_map[row[\"label\"]])  # Convert label to integer\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    images_tensor = torch.stack(images)  # Stack list of images into a single tensor\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "    return images_tensor, labels_tensor\n",
    "\n",
    "# Load train and validation sets\n",
    "train_images, train_labels = load_dataset(train_df, image_folder, transform)\n",
    "val_images, val_labels = load_dataset(val_df, image_folder, transform)\n",
    "\n",
    "\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "val_dataset = TensorDataset(val_images, val_labels)\n",
    "\n",
    "# Create dataloaders (process data in batches, reducing memory usage)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgnFobpMkwI4"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGpXP84zvoE7"
   },
   "source": [
    "## 4. Model developpment: CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOckMfuSqIsy"
   },
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDBWm6kkvs4S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3),  # -> 16x62x62\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                  # -> 16x31x31\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3), # -> 32x29x29\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                  # -> 32x14x14\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3), # -> 64x12x12\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                   # -> 64x6x6\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                     # -> (batch_size, 64 * 6 * 6)\n",
    "            nn.Linear(64 * 6 * 6, 512),        # = 2304 → 512\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)       # Final output = num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHEk33Vwk8zQ"
   },
   "outputs": [],
   "source": [
    "! pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMZMhMoqk--j"
   },
   "outputs": [],
   "source": [
    "# Print model summary : number of parameters, layers of the model\n",
    "from torchsummary import summary\n",
    "\n",
    "# Make sure input_size is defined like this (shape, not flattened!)\n",
    "input_size = (3, 64, 64)\n",
    "\n",
    "# Move model to device (if not already done)\n",
    "num_classes = len(train_df['label'].unique())\n",
    "model = CNNModel(num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print summary (input_size must be shape, not total pixels)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJgFRIBMuQHV"
   },
   "source": [
    "### Setting up MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute only if you have Databricks token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daZYG7-5uVeM"
   },
   "outputs": [],
   "source": [
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpo8HkYkuaPh"
   },
   "outputs": [],
   "source": [
    "# from random import random, randint\n",
    "# from mlflow import log_metric, log_param, log_artifacts\n",
    "# from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzIECfUludSj"
   },
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo2ydWGbufFo"
   },
   "outputs": [],
   "source": [
    "# # check databricks.txt\n",
    "# mlflow.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmpx-WhcuhEW"
   },
   "outputs": [],
   "source": [
    "# mlflow.set_tracking_uri(\"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmsZZRlvui_T"
   },
   "outputs": [],
   "source": [
    "# experiment_name = \"[put_your_link]/Pokemon_Classification_CNN_task3\"\n",
    "\n",
    "# existing_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# if existing_experiment is None:\n",
    "#     mlflow.create_experiment(\n",
    "#         experiment_name,\n",
    "#         artifact_location=\"dbfs:/Volumes/test/mlflow/Pokemon_Classification\",\n",
    "#     )\n",
    "\n",
    "# mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur5vEDalpwrV"
   },
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIlCHZQOpwOa"
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "num_classes = len(train_df['label'].unique())  # number of Pokémon classes\n",
    "model = CNNModel(num_classes)                  # pass only num_classes now\n",
    "model = model.to(device)                       # move to GPU (or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CV_mqjVjmOCR"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer definition\n",
    "criterion = nn.CrossEntropyLoss() #loss function\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001) #optimizer + lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7XhcLvHvgsD"
   },
   "outputs": [],
   "source": [
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxsX5WOZlk6S"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Loop parameters\n",
    "epochs = 100  # Maximum number of epochs #\n",
    "patience = 10  # Number of epochs to wait before stopping\n",
    "best_val_accuracy = 0  # Track the best validation accuracy\n",
    "early_stopping_epoch = 0  # Store the epoch where early stopping occurs\n",
    "counter = 0  # Count epochs without improvement\n",
    "\n",
    "# Store accuracies to plot later\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "mlflow.autolog()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, label in train_loader:\n",
    "        images, label = images.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "    # Calculate and log training accuracy\n",
    "    train_accuracy = correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # # uncomment to use MLFLow\n",
    "    # mlflow.log_metric(\"train_loss\", total_loss, step=epoch)\n",
    "    # mlflow.log_metric(\"train_acc\", train_accuracy, step=epoch)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, label in val_loader:\n",
    "            images, label = images.to(device), label.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actuals.extend(label.cpu().numpy())\n",
    "\n",
    "    # Compute validation accuracy and f1-score\n",
    "    val_accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    f1 = f1_score(actuals, predictions, average='macro')\n",
    "\n",
    "    # # uncomment to use MLFlow\n",
    "    # mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "    # mlflow.log_metric(\"val_f1_score\", f1, step=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Validation Accuracy: {val_accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if early_stop:\n",
    "      if val_accuracy > best_val_accuracy:\n",
    "          best_val_accuracy = val_accuracy\n",
    "          counter = 0\n",
    "      else:\n",
    "          counter += 1\n",
    "      if counter >= patience:\n",
    "          print(f\"Early stopping at epoch {epoch+1}\")\n",
    "          early_stopping_epoch = epoch + 1\n",
    "          break  # Stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4Y3WHyept_-"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy over the epochs\n",
    "plt.figure(figsize=(8, 5))\n",
    "epochs_range = range(1, len(train_accuracies) + 1)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Train Accuracy\", color='blue')\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\", color='red')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy (Early Stopping)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPOl7FypqPVT"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(actuals, predictions)\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "POKEMON_TYPES= train_df['label'].unique()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=POKEMON_TYPES, yticklabels=POKEMON_TYPES)\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EY9M5Wyqtcv"
   },
   "source": [
    "## 5. Some training metrics without using MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDMPgzo5quj4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store actual and predicted labels\n",
    "train_predictions = []\n",
    "train_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        train_predictions.extend(preds.cpu().numpy())\n",
    "        train_actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute classification report\n",
    "train_report = classification_report(train_actuals, train_predictions, digits=4)\n",
    "print(\"Training Set Metrics:\\n\", train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvudw3HXrIaC"
   },
   "outputs": [],
   "source": [
    "# Compute classification report\n",
    "val_report = classification_report(actuals, predictions, digits=4)\n",
    "print(\"Validation Set Metrics:\\n\", val_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8EXqkQZvNjH"
   },
   "source": [
    "## 6. Prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swjl32WcveAp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define image transformations (same as used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Ensure same size as training images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4464, 0.4480, 0.4158],[0.1823, 0.1728, 0.1813], inplace=False)\n",
    "])\n",
    "\n",
    "# Path to the folder containing test images\n",
    "test_folder = my_path + \"Test\"\n",
    "\n",
    "# Load test images\n",
    "test_images = [f for f in os.listdir(test_folder) if f.endswith(\".png\")]\n",
    "\n",
    "# Create label-to-index mapping\n",
    "unique_labels = train_df[\"label\"].unique()\n",
    "label_map = {Id: label for Id, label in enumerate(unique_labels)}\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store results\n",
    "submission_results = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for img_name in test_images:\n",
    "        img_path = os.path.join(test_folder, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(device)  # Add batch dimension & move to GPU\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class index\n",
    "\n",
    "        # Remove \".png\" extension and store result\n",
    "        img_id = img_name.replace(\".png\", \"\")\n",
    "        submission_results.append([img_id, label_map[predicted.item()]])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "submission_df = pd.DataFrame(submission_results, columns=[\"Id\", \"Label\"])\n",
    "\n",
    "# Save to CSV file\n",
    "submission_df.to_csv(\"../submission_CNN.csv\", index=False, header=True)\n",
    "\n",
    "print(\"Submission file 'submission_CNN.csv' has been created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
