{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8D3I-2ut6AT"
   },
   "source": [
    "# Pokémon images classification challenge: Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhXkY-Y6uMEn"
   },
   "source": [
    "## 1. Previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xc6kz4ZrjZ9"
   },
   "source": [
    "- Introduction\n",
    "- EDA (Exploratory Data Analysis) of the dataset\n",
    "- pre-processing of the data\n",
    "- classification using an MLP (Multilayer Perceptron).\n",
    "\n",
    "This work is available at this link: https://drive.google.com/file/d/1fci5SJnuwGc3tGgtzdE0X4BfafhX83B3/view?usp=sharing\n",
    "\n",
    "In this notebook, we perform a classification using a **CNN (Convolutional Neural Network)**. The dataset consists of **images of Pokémon**, each identified by an ID (corresponding to an image file with png extension) and a label indicating its **type**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaqGgcJ7rxsL"
   },
   "source": [
    "## 2. Setting up the environment and the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqfLDf78deXu"
   },
   "source": [
    "### Librairie importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ7kHl-pddy2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHjw3sqTsAU2"
   },
   "source": [
    "### Data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "executionInfo": {
     "elapsed": 5081,
     "status": "error",
     "timestamp": 1744571325635,
     "user": {
      "displayName": "Lina Lekbouri",
      "userId": "13756538654698639069"
     },
     "user_tz": -60
    },
    "id": "Tf2ST3e8d4bg",
    "outputId": "737c1e34-8c36-4153-e65c-d9cd463bdf94"
   },
   "outputs": [
    {
     "ename": "MessageError",
     "evalue": "Error: credential propagation was unsuccessful",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-52c4dd70687c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#use of google drive to import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
     ]
    }
   ],
   "source": [
    "# #use of google drive to import data\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1744571328795,
     "user": {
      "displayName": "Lina Lekbouri",
      "userId": "13756538654698639069"
     },
     "user_tz": -60
    },
    "id": "7Y9uoZQ7eWZi",
    "outputId": "65bb5e2d-4601-4902-fb4a-ce7a691117a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'drive/MyDrive/AP_Assignement1_Task2': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# ! ls drive/MyDrive/AP_Assignement1_Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1744571338480,
     "user": {
      "displayName": "Lina Lekbouri",
      "userId": "13756538654698639069"
     },
     "user_tz": -60
    },
    "id": "jvlbYd5HeDLJ"
   },
   "outputs": [],
   "source": [
    "#choosing the path where the dataset is stocked\n",
    "\n",
    "# # Google Colab\n",
    "# my_path = 'drive/MyDrive/AP_Assignement1_Task2/'\n",
    "\n",
    "# Kaggle\n",
    "# my_path = '/kaggle/input/the-pokemon-are-out-there-task-1/'\n",
    "\n",
    "# Local\n",
    "my_path = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdPOYYfYvmn9"
   },
   "source": [
    "## 3. Data prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liFPc3JVvk3M"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Set true to remove the background of the images in the Train folder\n",
    "# Set false to remove the background of the images in the Test folder\n",
    "ON_TRAINING = True\n",
    "\n",
    "def center_pokemon(image, thresholded_image):\n",
    "    \"\"\"\n",
    "    Center the Pokemon on it's picture\n",
    "\n",
    "    :param image_path: path the image on which we want to center the pokemon\n",
    "    :param thresholded_image_path: path to the image thresholded. This allow us\n",
    "    to get easily the dimension of the pokemon\n",
    "    :param output_path: path to where we want to save the picture\n",
    "\n",
    "    :return centered_image, centered_thresholded_image: The centered images\n",
    "    \"\"\"\n",
    "    # Invert the thresholded_image as we want the background to be black and the border white\n",
    "    binary = cv2.bitwise_not(thresholded_image)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if not contours:\n",
    "        print(\"No black element found!\")\n",
    "        return\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "    # Get image center\n",
    "    img_h, img_w = image.shape[:2]\n",
    "    center_x, center_y = img_w // 2, img_h // 2\n",
    "\n",
    "    # Compute translation needed to center the element\n",
    "    dx = center_x - (x + w // 2)\n",
    "    dy = center_y - (y + h // 2)\n",
    "\n",
    "    # Create translation matrix for RGB image\n",
    "    M = np.float32([[1, 0, dx], [0, 1, dy]])\n",
    "\n",
    "    # Apply translation to the RGB image\n",
    "    centered_image = cv2.warpAffine(image, M, (img_w, img_h), borderValue=(255, 255, 255))  # White border\n",
    "    centered_thresholded_image = cv2.warpAffine(thresholded_image, M, (img_w, img_h), borderValue=(255, 255, 255))\n",
    "\n",
    "    return centered_image,centered_thresholded_image\n",
    "\n",
    "\n",
    "def fix_border(image, save_path=None):\n",
    "    \"\"\"\n",
    "    Most of the pixels of the border are black (i.e their value is 1) but few of them\n",
    "    are grey (i.e their value is 32) so we make sure all the pixels are black.\n",
    "    This will be helpful when we will apply the threshold to only keep the border.\n",
    "\n",
    "    :param image: The input image in cv2 format and greyscale.\n",
    "    :param save_path: Path to save the processed image.\n",
    "\n",
    "    :return: The processed image in cv2 format.\n",
    "    \"\"\"\n",
    "    height, width = image.shape\n",
    "    processed_image = image.copy()\n",
    "\n",
    "    for y in range(1, height - 1):\n",
    "        for x in range(1, width - 1):\n",
    "            if image[y, x] == 32:\n",
    "                if (image[y - 1, x] == 1 or image[y + 1, x] == 1 or  # Vertical neighbors\n",
    "                        image[y, x - 1] == 1 or image[y, x + 1] == 1 or  # Horizontal neighbors\n",
    "                        image[y - 1, x - 1] == 1 or image[y - 1, x + 1] == 1 or  # Top diagonals\n",
    "                        image[y + 1, x - 1] == 1 or image[y + 1, x + 1] == 1):  # Bottom diagonals\n",
    "                    processed_image[y, x] = 1\n",
    "\n",
    "    if(save_path):\n",
    "        cv2.imwrite(save_path, processed_image)\n",
    "\n",
    "    return processed_image\n",
    "\n",
    "\n",
    "def apply_threshold(image, save_path=None):\n",
    "    \"\"\"\n",
    "    Apply a threshold to only keep the border.\n",
    "\n",
    "    :param image: The image to which the threshold will be applied. Must be in cv2 format and greyscale.\n",
    "    :param save_path: Path to save the processed image.\n",
    "\n",
    "    :return: The threshold image cv2 format.\n",
    "    \"\"\"\n",
    "\n",
    "    _, binary = cv2.threshold(image, 5, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, binary)\n",
    "\n",
    "    return binary\n",
    "\n",
    "\n",
    "def erode_and_dilate(image, save_path=None):\n",
    "    \"\"\"\n",
    "    Apply morphological erosion and dilatation to an image.\n",
    "    Now that we applied the threshold to only keep the border, we can apply this in order\n",
    "    to \"fill\" the inside of the border.\n",
    "    Once the operations are applied, we get a mask that we can apply on the initial image\n",
    "\n",
    "    :param image: The image to which the operations will be applied. Must be in cv2 format and greyscale.\n",
    "    :param save_path: Path to save the processed image.\n",
    "\n",
    "    :return: The image cv2 format after applying the erosion and dilatation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the area on which we apply the operation for each pixel\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "\n",
    "    # We may need to adjust the number of iterations\n",
    "    # I experience few values and it seems that 9 is the best I found so far\n",
    "    image = cv2.erode(image, kernel, iterations=10)\n",
    "    image = cv2.dilate(image, kernel, iterations=10)\n",
    "\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, image)\n",
    "    return image\n",
    "\n",
    "def apply_mask(image, mask, save_path=None):\n",
    "    \"\"\"\n",
    "    Apply the mask we created on the initial image in order to remove the background.\n",
    "\n",
    "    :param image: The image to which the mask will be applied.\n",
    "    :param mask: The mask to apply on the initial image.\n",
    "    :param save_path: Path to save the processed image.\n",
    "\n",
    "    :return: The image cv2 format after applying the mask.\n",
    "    If everything went fine, most of the background should have been removed.\n",
    "    \"\"\"\n",
    "\n",
    "    inverted_mask = cv2.bitwise_not(mask)\n",
    "    masked_image_inverted = cv2.bitwise_and(image, image, mask=inverted_mask)\n",
    "\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, masked_image_inverted)\n",
    "\n",
    "    return masked_image_inverted\n",
    "\n",
    "def isolate_pokemon(image_name):\n",
    "    \"\"\"\n",
    "    Apply every operation we need to remove the background so we can isolate the Pokemon.\n",
    "    Once the operations are completed, save the processed image in the folder \"Test_processed\"\n",
    "    with the same name.\n",
    "\n",
    "    :param image_name: The name of the image in which we want to isolate the Pokemon.\n",
    "\n",
    "    \"\"\"\n",
    "    if(ON_TRAINING):\n",
    "        image = cv2.imread(my_path + \"Train/\" + str(image_name))\n",
    "        grey_image = cv2.imread(my_path + \"Train/\" + str(image_name), cv2.IMREAD_GRAYSCALE)\n",
    "    else:\n",
    "        image = cv2.imread(my_path + \"Test/\" + str(image_name))\n",
    "        grey_image = cv2.imread(my_path + \"Test/\" + str(image_name), cv2.IMREAD_GRAYSCALE)\n",
    "    grey_image = fix_border(grey_image)\n",
    "    grey_image = apply_threshold(grey_image, \"thresholded.png\")\n",
    "    centered_image, grey_centered_image = center_pokemon(image,grey_image)\n",
    "    grey_centered_image = erode_and_dilate(grey_centered_image)\n",
    "    centered_image = apply_mask(centered_image,grey_centered_image)\n",
    "    if(ON_TRAINING):\n",
    "        if not os.path.exists(my_path + \"Train_processed\"):\n",
    "            os.mkdir(my_path + \"Train_processed\")\n",
    "        cv2.imwrite(my_path + \"Train_processed/\" + str(image_name), centered_image)\n",
    "    else:\n",
    "        if not os.path.exists(my_path + \"Test_processed\"):\n",
    "            os.mkdir(my_path + \"Test_processed\")\n",
    "        cv2.imwrite(my_path + \"Test_processed/\" + str(image_name), centered_image)\n",
    "\n",
    "def isolate_every_pokemon():\n",
    "    \"\"\"\n",
    "    Remove the background for every picture in the folder \"Test\" and save the processed image\n",
    "    in the folder \"Test_processed\"\n",
    "    \"\"\"\n",
    "    if(ON_TRAINING):\n",
    "        if not os.path.exists(my_path + \"Train\"):\n",
    "            print(\"The folder 'Train' does not exist.\")\n",
    "        else:\n",
    "            for image_name in os.listdir(my_path + \"Train\"):\n",
    "                isolate_pokemon(image_name)\n",
    "    else:\n",
    "        if not os.path.exists(my_path + \"Test\"):\n",
    "            print(\"The folder 'Test' does not exist.\")\n",
    "        else:\n",
    "            for image_name in os.listdir(my_path + \"Test\"):\n",
    "                isolate_pokemon(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A-Q6AFWnS_A8"
   },
   "outputs": [],
   "source": [
    "isolate_every_pokemon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snYOJmYZf9-N"
   },
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-odVZk3XgAnC"
   },
   "outputs": [],
   "source": [
    "# Load labels\n",
    "data_path = my_path +\"train_labels.csv\"\n",
    "image_folder = my_path +\"Train_processed\"\n",
    "labels_df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1744063721233,
     "user": {
      "displayName": "Hugo Werck",
      "userId": "15023081001397889950"
     },
     "user_tz": -60
    },
    "id": "iumqx2hdggiX",
    "outputId": "51af87f3-0dc2-4f01-8d1c-3effe342ec0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-219a7947c3f6>:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  balanced_df = labels_df.groupby('label').apply(lambda x: x.sample(min_class_count)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "#undersampling to balance the data\n",
    "min_class_count = labels_df['label'].value_counts().min()\n",
    "balanced_df = labels_df.groupby('label').apply(lambda x: x.sample(min_class_count)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_40PxIUTjcQ-"
   },
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "train_df, val_df = train_test_split(balanced_df, test_size=0.2, stratify=balanced_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSXsHx1Ijfd1"
   },
   "outputs": [],
   "source": [
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), #assure all images are 64x64\n",
    "    transforms.ToTensor(), #convert to tensor pytorch\n",
    "    transforms.Normalize([0.4464, 0.4480, 0.4158],[0.1823, 0.1728, 0.1813], inplace=False),\n",
    "    transforms.RandomHorizontalFlip(), #data augmentation, we don't use the crop because of our data preprocessing, since the crop is random it can remove the pokémon\n",
    "    #and affects our performance\n",
    "    transforms.RandomRotation(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 592093,
     "status": "error",
     "timestamp": 1744064313387,
     "user": {
      "displayName": "Hugo Werck",
      "userId": "15023081001397889950"
     },
     "user_tz": -60
    },
    "id": "E83VDJ48kX5K",
    "outputId": "b1c69bac-626a-419b-a33e-c7441042001f"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-861f1adc4408>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Load train and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mval_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-861f1adc4408>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(df, img_folder, transform)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".png\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Construct full image path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Open image in RGB mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply transformations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3474\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3476\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "\n",
    "# Create label-to-index mapping\n",
    "unique_labels = train_df[\"label\"].unique()\n",
    "label_map = {label: Id for Id, label in enumerate(unique_labels)}\n",
    "\n",
    "# Function to load images and labels into tensors\n",
    "def load_dataset(df, img_folder, transform):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        img_path = os.path.join(img_folder, row[\"Id\"] + \".png\")  # Construct full image path\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Open image in RGB mode\n",
    "        image = transform(image)  # Apply transformations\n",
    "\n",
    "        images.append(image)\n",
    "        labels.append(label_map[row[\"label\"]])  # Convert label to integer\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    images_tensor = torch.stack(images)  # Stack list of images into a single tensor\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.long)  # Convert labels to tensor\n",
    "\n",
    "    return images_tensor, labels_tensor\n",
    "\n",
    "# Load train and validation sets\n",
    "train_images, train_labels = load_dataset(train_df, image_folder, transform)\n",
    "val_images, val_labels = load_dataset(val_df, image_folder, transform)\n",
    "\n",
    "\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "val_dataset = TensorDataset(val_images, val_labels)\n",
    "\n",
    "# Create dataloaders (process data in batches, reducing memory usage)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgnFobpMkwI4"
   },
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGpXP84zvoE7"
   },
   "source": [
    "## 4. Model developpment: CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOckMfuSqIsy"
   },
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDBWm6kkvs4S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3),  # -> 16x62x62\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                  # -> 16x31x31\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3), # -> 32x29x29\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                  # -> 32x14x14\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3), # -> 64x12x12\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                   # -> 64x6x6\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),                     # -> (batch_size, 64 * 6 * 6)\n",
    "            nn.Linear(64 * 6 * 6, 512),        # = 2304 → 512\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)       # Final output = num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHEk33Vwk8zQ"
   },
   "outputs": [],
   "source": [
    "! pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMZMhMoqk--j"
   },
   "outputs": [],
   "source": [
    "# Print model summary : number of parameters, layers of the model\n",
    "from torchsummary import summary\n",
    "\n",
    "# Make sure input_size is defined like this (shape, not flattened!)\n",
    "input_size = (3, 64, 64)\n",
    "\n",
    "# Move model to device (if not already done)\n",
    "num_classes = len(train_df['label'].unique())\n",
    "model = CNNModel(num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print summary (input_size must be shape, not total pixels)\n",
    "summary(model, input_size=input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJgFRIBMuQHV"
   },
   "source": [
    "### Setting up MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To execute only if you have Databricks token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daZYG7-5uVeM"
   },
   "outputs": [],
   "source": [
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qpo8HkYkuaPh"
   },
   "outputs": [],
   "source": [
    "# from random import random, randint\n",
    "# from mlflow import log_metric, log_param, log_artifacts\n",
    "# from mlflow.tracking import MlflowClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FzIECfUludSj"
   },
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# import mlflow.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo2ydWGbufFo"
   },
   "outputs": [],
   "source": [
    "# # check databricks.txt\n",
    "# mlflow.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmpx-WhcuhEW"
   },
   "outputs": [],
   "source": [
    "# mlflow.set_tracking_uri(\"databricks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmsZZRlvui_T"
   },
   "outputs": [],
   "source": [
    "# experiment_name = \"[put your link]/Pokemon_Classification_CNN\"\n",
    "\n",
    "# existing_experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# if existing_experiment is None:\n",
    "#     mlflow.create_experiment(\n",
    "#         experiment_name,\n",
    "#         artifact_location=\"dbfs:/Volumes/test/mlflow/Pokemon_Classification\",\n",
    "#     )\n",
    "\n",
    "# mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ur5vEDalpwrV"
   },
   "source": [
    "### Train and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pIlCHZQOpwOa"
   },
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "num_classes = len(train_df['label'].unique())  # number of Pokémon classes\n",
    "model = CNNModel(num_classes)                  # pass only num_classes now\n",
    "model = model.to(device)                       # move to GPU (or CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CV_mqjVjmOCR"
   },
   "outputs": [],
   "source": [
    "# Loss and optimizer definition\n",
    "criterion = nn.CrossEntropyLoss() #loss function\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001) #optimizer + lr = learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7XhcLvHvgsD"
   },
   "outputs": [],
   "source": [
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxsX5WOZlk6S"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate model\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Loop parameters\n",
    "epochs = 100  # Maximum number of epochs #\n",
    "patience = 10  # Number of epochs to wait before stopping\n",
    "best_val_accuracy = 0  # Track the best validation accuracy\n",
    "early_stopping_epoch = 0  # Store the epoch where early stopping occurs\n",
    "counter = 0  # Count epochs without improvement\n",
    "\n",
    "# Store accuracies to plot later\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # mlflow.autolog()\n",
    "\n",
    "    # Training mode\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, label in train_loader:\n",
    "        images, label = images.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate statistics\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        total += label.size(0)\n",
    "\n",
    "    # Calculate and log training accuracy\n",
    "    train_accuracy = correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # # uncomment to use MLFLow\n",
    "    # mlflow.log_metric(\"train_loss\", total_loss, step=epoch)\n",
    "    # mlflow.log_metric(\"train_acc\", train_accuracy, step=epoch)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, label in val_loader:\n",
    "            images, label = images.to(device), label.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actuals.extend(label.cpu().numpy())\n",
    "\n",
    "    # Compute validation accuracy and f1-score\n",
    "    val_accuracy = np.mean(np.array(predictions) == np.array(actuals))\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    f1 = f1_score(actuals, predictions, average='macro')\n",
    "\n",
    "    # # uncomment to use MLFlow\n",
    "    # mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "    # mlflow.log_metric(\"val_f1_score\", f1, step=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "          f\"Validation Accuracy: {val_accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if early_stop:\n",
    "      if val_accuracy > best_val_accuracy:\n",
    "          best_val_accuracy = val_accuracy\n",
    "          counter = 0\n",
    "      else:\n",
    "          counter += 1\n",
    "      if counter >= patience:\n",
    "          print(f\"Early stopping at epoch {epoch+1}\")\n",
    "          early_stopping_epoch = epoch + 1\n",
    "          break  # Stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4Y3WHyept_-"
   },
   "outputs": [],
   "source": [
    "# Plot the accuracy over the epochs\n",
    "plt.figure(figsize=(8, 5))\n",
    "epochs_range = range(1, len(train_accuracies) + 1)\n",
    "plt.plot(epochs_range, train_accuracies, label=\"Train Accuracy\", color='blue')\n",
    "plt.plot(epochs_range, val_accuracies, label=\"Validation Accuracy\", color='red')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training vs Validation Accuracy (Early Stopping)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPOl7FypqPVT"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(actuals, predictions)\n",
    "print(conf_matrix)\n",
    "# Plot confusion matrix\n",
    "POKEMON_TYPES= train_df['label'].unique()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", xticklabels=POKEMON_TYPES, yticklabels=POKEMON_TYPES)\n",
    "plt.xlabel(\"Predictions\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EY9M5Wyqtcv"
   },
   "source": [
    "## 5. Some training metrics without using MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDMPgzo5quj4"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store actual and predicted labels\n",
    "train_predictions = []\n",
    "train_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        train_predictions.extend(preds.cpu().numpy())\n",
    "        train_actuals.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute classification report\n",
    "train_report = classification_report(train_actuals, train_predictions, digits=4)\n",
    "print(\"Training Set Metrics:\\n\", train_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvudw3HXrIaC"
   },
   "outputs": [],
   "source": [
    "# Compute classification report\n",
    "val_report = classification_report(actuals, predictions, digits=4)\n",
    "print(\"Validation Set Metrics:\\n\", val_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8EXqkQZvNjH"
   },
   "source": [
    "## 6. Prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptFY59M-vTWG"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the test data\n",
    "ON_TRAINING = False\n",
    "isolate_every_pokemon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "swjl32WcveAp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define image transformations (same as used during training)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Ensure same size as training images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4464, 0.4480, 0.4158],[0.1823, 0.1728, 0.1813], inplace=False)\n",
    "])\n",
    "\n",
    "# Path to the folder containing test images\n",
    "test_folder = my_path + \"Test_processed\"\n",
    "\n",
    "# Load test images\n",
    "test_images = [f for f in os.listdir(test_folder) if f.endswith(\".png\")]\n",
    "\n",
    "# Create label-to-index mapping\n",
    "unique_labels = train_df[\"label\"].unique()\n",
    "label_map = {Id: label for Id, label in enumerate(unique_labels)}\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Store results\n",
    "submission_results = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for img_name in test_images:\n",
    "        img_path = os.path.join(test_folder, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = transform(image).unsqueeze(0).to(device)  # Add batch dimension & move to GPU\n",
    "\n",
    "        # Get model predictions\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted class index\n",
    "\n",
    "        # Remove \".png\" extension and store result\n",
    "        img_id = img_name.replace(\".png\", \"\")\n",
    "        submission_results.append([img_id, label_map[predicted.item()]])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "submission_df = pd.DataFrame(submission_results, columns=[\"Id\", \"Label\"])\n",
    "\n",
    "# Save to CSV file\n",
    "submission_df.to_csv(\"../submission_task2.csv\", index=False, header=True)\n",
    "\n",
    "print(\"Submission file 'submission_CNN.csv' has been created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
